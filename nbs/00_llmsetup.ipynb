{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llmsetup\n",
    "\n",
    "> Experiments using the Python LLM model and local models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Python LLM package, and local models\n",
    "\n",
    "This notebook is meant to be a playground for exploring the Python LLM package by [Simon Willison](\n",
    "Simon Willison) for managing local LLM models as well as remote large language models like OpenAI's GPT-3. The LLM package supports both [llama-cpp](https://github.com/ggerganov/llama.cpp) and [Machine Learning Compilation (MLC)](https://huggingface.co/mlc-ai). A large number of modelsfor llama-cpp are available from [HuggingFace Tom Jobbins repository](https://huggingface.co/TheBloke) using [GGML](https://github.com/ggerganov/ggml) quantization.\n",
    "\n",
    "\n",
    "The following resources are useful in getting llm up and running.\n",
    "\n",
    "- [llm github repo](https://github.com/simonw/llm)\n",
    "- [llm plugins](https://llm.datasette.io/en/stable/plugins/directory.html#plugin-directory)\n",
    "- [llm-mlc plugin](https://github.com/simonw/llm-mlc)\n",
    "- [llm documentation](https://llm.datasette.io/en/stable/index.html#)\n",
    "- [llm llama-cpp](https://github.com/simonw/llm-llama-cpp)\n",
    "- [MLC llm](https://mlc.ai/mlc-llm/)\n",
    "- [MLC llm github repo](https://github.com/mlc-ai/mlc-llm)\n",
    "- [MLC supported models on Huggingface](https://huggingface.co/mlc-ai)\n",
    "- [MLC Tutorial for Getting Started](https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb)\n",
    "\n",
    "\n",
    "Jeremy Howard has a nice introduction to LLMs in Kaggle\n",
    "[Getting Started With LLMs](https://www.kaggle.com/code/jhoward/getting-started-with-llms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM installation\n",
    "The remote LLMs from huggingface use git-lfs to store the models. For a mac, use [homebrew](https://brew.sh/) to install git-lfs.\n",
    "\n",
    "```bash\n",
    "brew install git-lfs\n",
    "```\n",
    "\n",
    "Install the llm package and the llm-mlc plugin.\n",
    "```bash\n",
    "llm install llm-mlc\n",
    "llm mlc pip install --pre --force-reinstall \\\n",
    "  mlc-ai-nightly \\\n",
    "  mlc-chat-nightly \\\n",
    "  -f https://mlc.ai/wheels\n",
    "llm mlc setup\n",
    "llm mlc download-model Llama-2-13b-chat --alias l13b\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp llmsetup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are five cute names for a pet llama:\n",
      "\n",
      "1. Luna (moon-like in appearance and graceful)\n",
      "2. Cuddles (perfect for a snuggly llama)\n",
      "3. Peanut (for a small, adorable llama)\n",
      "4. Fluffy (for a llama with a soft, fluffy coat)\n",
      "5. Sprinkles (for a llama with a unique, spotted appearance)\n",
      "\n",
      "I hope these names inspire you and help you find the perfect one for your pet llama!\n"
     ]
    }
   ],
   "source": [
    "import llm\n",
    "model = llm.get_model(\"l13b\")\n",
    "response = model.prompt(\n",
    "    \"Give Five cute names for a pet lamma\",\n",
    "    system=\"You are an expert at naming llamas\",\n",
    ")\n",
    "print(response.text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Fluffy\n",
      "2. Caramel\n",
      "3. Cotton\n",
      "4. Peanut\n",
      "5. Marshmallow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bigmodel = llm.get_model(\"gpt-3.5-turbo\")\n",
    "bigmodel.key = os.environ['OPENAI_API_KEY']\n",
    "response = bigmodel.prompt(\"Give Five cute names for a pet lamma\")\n",
    "print(response.text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\n",
      "OpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\n",
      "OpenAI Chat: gpt-4 (aliases: 4, gpt4)\n",
      "OpenAI Chat: gpt-4-32k (aliases: 4-32k)\n",
      "MlcModel: mlc-chat-Llama-2-13b-chat-hf-q4f16_1 (aliases: l13b, Llama-2-13b-chat)\n"
     ]
    }
   ],
   "source": [
    "!llm models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = model.conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Chuck! It's nice to meet you. Is there something specific you would like to know or talk about? As a helpful and respectful assistant, I'm here to provide information and answer questions to the best of my ability. Please keep in mind that I am not able to provide harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. All questions and answers should be socially unbiased and positive in nature. If a question does not make sense or is not factually coherent, I will do my best to explain why. Please feel free to ask me anything!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.prompt(\"Hello I'm Chuck.\").text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ASSISTANT Hello Chuck! I do not have the ability to remember previous conversations or store information about users. Each time you interact with me, it is a new and independent conversation. However, I am here to help answer any questions you may have to the best of my ability. Is there something specific you would like to know or discuss? Please keep in mind that I am not able to provide harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. All questions and answers should be socially unbiased and positive in nature.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.prompt(\"Do you remember my name?\").text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigconv = bigmodel.conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Chuck! How can I assist you today?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigconv.prompt(\"Hello I'm Chuck.\").text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, I do remember your name. Your name is Chuck. How can I help you today, Chuck?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigconv.prompt(\"Do you remember my name?\").text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
