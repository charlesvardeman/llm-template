{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llmsetup\n",
    "\n",
    "> Experiments using the Python LLM model and local models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Python LLM package, and local models\n",
    "\n",
    "This notebook is meant to be a playground for exploring the Python LLM package by [Simon Willison](\n",
    "Simon Willison) for managing local LLM models as well as remote large language models like OpenAI's GPT-3. The LLM package supports both [llama-cpp](https://github.com/ggerganov/llama.cpp) and [Machine Learning Compilation (MLC)](https://huggingface.co/mlc-ai). A large number of modelsfor llama-cpp are available from [HuggingFace Tom Jobbins repository](https://huggingface.co/TheBloke) using [GGML](https://github.com/ggerganov/ggml) quantization.\n",
    "\n",
    "\n",
    "The following resources are useful in getting llm up and running.\n",
    "\n",
    "- [llm github repo](https://github.com/simonw/llm)\n",
    "- [llm plugins](https://llm.datasette.io/en/stable/plugins/directory.html#plugin-directory)\n",
    "- [llm-mlc plugin](https://github.com/simonw/llm-mlc)\n",
    "- [llm documentation](https://llm.datasette.io/en/stable/index.html#)\n",
    "- [llm llama-cpp](https://github.com/simonw/llm-llama-cpp)\n",
    "- [MLC llm](https://mlc.ai/mlc-llm/)\n",
    "- [MLC llm github repo](https://github.com/mlc-ai/mlc-llm)\n",
    "- [MLC supported models on Huggingface](https://huggingface.co/mlc-ai)\n",
    "- [MLC Tutorial for Getting Started](https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb)\n",
    "\n",
    "\n",
    "Jeremy Howard has a nice introduction to LLMs in Kaggle\n",
    "[Getting Started With LLMs](https://www.kaggle.com/code/jhoward/getting-started-with-llms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM installation\n",
    "The remote LLMs from huggingface use git-lfs to store the models. For a mac, use [homebrew](https://brew.sh/) to install git-lfs.\n",
    "\n",
    "```bash\n",
    "brew install git-lfs\n",
    "```\n",
    "\n",
    "Install the llm package and the llm-mlc plugin.\n",
    "```bash\n",
    "llm install llm-mlc\n",
    "llm mlc pip install --pre --force-reinstall \\\n",
    "  mlc-ai-nightly \\\n",
    "  mlc-chat-nightly \\\n",
    "  -f https://mlc.ai/wheels\n",
    "llm mlc setup\n",
    "llm mlc download-model Llama-2-13b-chat --alias l13b\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp llmsetup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are five cute names for a pet llama:\n",
      "\n",
      "1. Luna (moon-like in appearance and graceful)\n",
      "2. Cuddles (perfect for a snuggly llama)\n",
      "3. Peanut (for a small, adorable llama)\n",
      "4. Fluffy (for a llama with a soft, fluffy coat)\n",
      "5. Sprinkles (for a llama with a unique, spotted appearance)\n",
      "\n",
      "I hope these names inspire you and help you find the perfect one for your pet llama!\n"
     ]
    }
   ],
   "source": [
    "import llm\n",
    "model = llm.get_model(\"l13b\")\n",
    "response = model.prompt(\n",
    "    \"Give Five cute names for a pet lamma\",\n",
    "    system=\"You are an expert at naming llamas\",\n",
    ")\n",
    "print(response.text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Fluffy\n",
      "2. Caramel\n",
      "3. Cotton\n",
      "4. Peanut\n",
      "5. Marshmallow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bigmodel = llm.get_model(\"gpt-3.5-turbo\")\n",
    "bigmodel.key = os.environ['OPENAI_API_KEY']\n",
    "response = bigmodel.prompt(\"Give Five cute names for a pet lamma\")\n",
    "print(response.text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\n",
      "OpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\n",
      "OpenAI Chat: gpt-4 (aliases: 4, gpt4)\n",
      "OpenAI Chat: gpt-4-32k (aliases: 4-32k)\n",
      "MlcModel: mlc-chat-Llama-2-13b-chat-hf-q4f16_1 (aliases: l13b, Llama-2-13b-chat)\n"
     ]
    }
   ],
   "source": [
    "!llm models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = model.conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Chuck! It's nice to meet you. Is there something specific you would like to know or talk about? As a helpful and respectful assistant, I'm here to provide information and answer questions to the best of my ability. Please keep in mind that I am not able to provide harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. All questions and answers should be socially unbiased and positive in nature. If a question does not make sense or is not factually coherent, I will do my best to explain why. Please feel free to ask me anything!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.prompt(\"Hello I'm Chuck.\").text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ASSISTANT Hello Chuck! I do not have the ability to remember previous conversations or store information about users. Each time you interact with me, it is a new and independent conversation. However, I am here to help answer any questions you may have to the best of my ability. Is there something specific you would like to know or discuss? Please keep in mind that I am not able to provide harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. All questions and answers should be socially unbiased and positive in nature.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.prompt(\"Do you remember my name?\").text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigconv = bigmodel.conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Chuck! How can I assist you today?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigconv.prompt(\"Hello I'm Chuck.\").text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, I do remember your name. Your name is Chuck. How can I help you today, Chuck?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigconv.prompt(\"Do you remember my name?\").text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are five cute names for a pet llama:\n",
      "\n",
      "1. Luna - This name is perfect for a llama because it means \"moon\" in Spanish, and llamas are known for their celestial-sounding calls. It's also a cute and playful name that suits a friendly and curious pet llama.\n",
      "2. Coco - This name is a playful take on the popular llama name \"Coconut.\" It's a fun and catchy name that suits a llama with a sweet and playful personality.\n",
      "3. Lola - This name is a diminutive form of \"Dolores,\" which means \"sorrows\" in Spanish. It's a cute and endearing name that suits a llama with a gentle and affectionate personality.\n",
      "4. Peanut - This name is perfect for a small or young llama, as it's a playful way to refer to their size. It's also a cute and affectionate name that suits a llama with a friendly and outgoing personality.\n",
      "5. Sprinkles - This name is perfect for a llama with a playful and fun-loving personality. It's a cute and whimsical name that suits a llama with a colorful and lively appearance, such as a llama with a sprinkle of color on their fur.\n",
      "\n",
      "I hope these suggestions help inspire you to find the perfect name for your pet llama!\n"
     ]
    }
   ],
   "source": [
    "bigllama = llm.get_model(\"l70b\")\n",
    "response = bigllama.prompt(\n",
    "    \"Give Five cute names for a pet lamma\",\n",
    "    system=\"You are an expert at naming llamas\",\n",
    ")\n",
    "print(response.text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Chuck, it's nice to meet you. Is there anything I can help you with or would you like to chat about something in particular?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigllamaconv = bigllama.conversation()\n",
    "bigllamaconv.prompt(\"Hello I'm Chuck.\").text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, I remember your name, Chuck. It's nice to meet you too! Is there anything specific you'd like to chat about or ask me? I'm here to help with any questions you might have.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigllamaconv.prompt(\"Do you remember my name?\").text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct some sample json-ld data as a string\n",
    "jsonlddata = \"\"\" \n",
    "{ @context: \"http://schema.org/\",\n",
    "    @type: \"Person\",\n",
    "    name: \"Jane Doe\",\n",
    "    jobTitle: \"Professor\",\n",
    "    telephone: \"(425) 123-4567\",\n",
    "    url: \"http://www.janedoe.com\" }\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, I can help you interpret the JSON-LD data you provided.\\n\\nThis JSON-LD data defines a Person entity with the following properties:\\n\\n* @context: This specifies the schema.org vocabulary used in the JSON-LD data.\\n* @type: This specifies the type of entity being described, in this case, a Person.\\n* name: This specifies the person\\'s name, which is \"Jane Doe\" in this case.\\n* jobTitle: This specifies the person\\'s job title, which is \"Professor\" in this case.\\n* telephone: This specifies the person\\'s telephone number, which is \"(425) 123-4567\" in this case.\\n* url: This specifies the person\\'s website URL, which is \"http://www.janedoe.com\" in this case.\\n\\nIn summary, this JSON-LD data describes a Person entity named Jane Doe, who is a Professor and can be contacted through her telephone number or website URL.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigllamaconv.prompt(\"Can you interpret this JSON-LD Data: \" + jsonlddata ).text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is an RDF dataset that describes a set of concepts and their relationships. It appears to be a vocabulary for describing maintenance and repair operations, with concepts related to equipment, components, maintenance tasks, and trouble codes.\\n\\nHere are some observations and potential issues with the dataset:\\n\\n1. The dataset contains a mix of concept types, including both object properties (e.g., \"equipment\") and datatype properties (e.g., \"statusCode\"). It\\'s not clear how these different types of concepts are related or how they should be used together.\\n2. Many of the concepts have'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigllamaconv.prompt(\"Can you interpret this JSON-LD Data: \" + twokdata ).text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 7775 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bigconv\u001b[39m.\u001b[39;49mprompt(\u001b[39m\"\u001b[39;49m\u001b[39mCan you interpret this JSON-LD Data: \u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m twokdata )\u001b[39m.\u001b[39;49mtext()\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-template/lib/python3.11/site-packages/llm/models.py:108\u001b[0m, in \u001b[0;36mResponse.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtext\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force()\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunks)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-template/lib/python3.11/site-packages/llm/models.py:105\u001b[0m, in \u001b[0;36mResponse._force\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_force\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    104\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_done:\n\u001b[0;32m--> 105\u001b[0m         \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-template/lib/python3.11/site-packages/llm/models.py:90\u001b[0m, in \u001b[0;36mResponse.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_done:\n\u001b[1;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunks\n\u001b[0;32m---> 90\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mexecute(\n\u001b[1;32m     91\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt,\n\u001b[1;32m     92\u001b[0m     stream\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream,\n\u001b[1;32m     93\u001b[0m     response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m     94\u001b[0m     conversation\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconversation,\n\u001b[1;32m     95\u001b[0m ):\n\u001b[1;32m     96\u001b[0m     \u001b[39myield\u001b[39;00m chunk\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunks\u001b[39m.\u001b[39mappend(chunk)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-template/lib/python3.11/site-packages/llm/default_plugins/openai_models.py:227\u001b[0m, in \u001b[0;36mChat.execute\u001b[0;34m(self, prompt, stream, response, conversation)\u001b[0m\n\u001b[1;32m    225\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaders\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m--> 227\u001b[0m     completion \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    228\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_id,\n\u001b[1;32m    229\u001b[0m         messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m    230\u001b[0m         stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    231\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    232\u001b[0m     )\n\u001b[1;32m    233\u001b[0m     chunks \u001b[39m=\u001b[39m []\n\u001b[1;32m    234\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m completion:\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-template/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-template/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-template/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-template/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-template/lib/python3.11/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 7775 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "bigconv.prompt(\"Can you interpret this JSON-LD Data: \" + twokdata ).text()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
